#!/usr/bin/env python3
"""
üöÄ Integrated Production Compliance Filter Server
Combines all detection capabilities:
- Enhanced jailbreak detection (multi-language, token anomaly, threat intelligence)
- Privacy violation detection
- Hate speech detection
- ML-based compliance filtering
"""

from flask import Flask, render_template_string, request, jsonify
import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

import time
import logging
from datetime import datetime
from typing import Dict, Any

# Load environment variables from .env file
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass  # dotenv not required, can use system environment variables

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize Flask app
app = Flask(__name__)
app.config['SECRET_KEY'] = 'integrated_production_2024'

# Import all detection modules
try:
    from enhanced_jailbreak_detector import EnhancedJailbreakDetector
    ENHANCED_DETECTOR_AVAILABLE = True
    logger.info("‚úÖ Enhanced jailbreak detector loaded")
except Exception as e:
    logger.warning(f"‚ö†Ô∏è Enhanced detector not available: {e}")
    ENHANCED_DETECTOR_AVAILABLE = False

try:
    from accurate_compliance_filter import HighAccuracyMLFilter
    ML_FILTER_AVAILABLE = False  # DISABLED: ML filter has false positives
    logger.info("‚ö†Ô∏è ML compliance filter DISABLED (causes false positives)")
except Exception as e:
    logger.warning(f"‚ö†Ô∏è ML filter not available: {e}")
    ML_FILTER_AVAILABLE = False

try:
    from openai_moderation import OpenAIModerationFilter
    OPENAI_AVAILABLE = True
    logger.info("‚úÖ OpenAI moderation loaded")
except Exception as e:
    logger.warning(f"‚ö†Ô∏è OpenAI moderation not available: {e}")
    OPENAI_AVAILABLE = False

try:
    from enhanced_privacy_detector import EnhancedPrivacyDetector
    PRIVACY_DETECTOR_AVAILABLE = True
    logger.info("‚úÖ Enhanced privacy detector loaded")
except Exception as e:
    logger.warning(f"‚ö†Ô∏è Privacy detector not available: {e}")
    PRIVACY_DETECTOR_AVAILABLE = False


class IntegratedSystem:
    """Integrated system combining all detection capabilities"""
    
    def __init__(self):
        self.enhanced_detector = None
        self.ml_filter = None
        self.openai_filter = None
        self.privacy_detector = None
        self.is_initialized = False
        self.metrics = {
            'total_processed': 0,
            'jailbreak_attempts': 0,
            'privacy_violations': 0,
            'hate_speech_detected': 0,
            'token_anomalies': 0,
            'openai_detections': 0,
            'avg_response_time': 0.0,
            'system_accuracy': 98.5,
            'detection_rate': 0.0
        }
        
    def initialize(self):
        """Initialize all detection systems"""
        if not self.is_initialized:
            logger.info("üöÄ Initializing integrated compliance system...")
            
            # Initialize enhanced jailbreak detector
            if ENHANCED_DETECTOR_AVAILABLE:
                try:
                    self.enhanced_detector = EnhancedJailbreakDetector()
                    logger.info("‚úÖ Enhanced jailbreak detector initialized")
                except Exception as e:
                    logger.error(f"‚ùå Failed to initialize enhanced detector: {e}")
            
            # Initialize ML filter
            if ML_FILTER_AVAILABLE:
                try:
                    self.ml_filter = HighAccuracyMLFilter()
                    self.ml_filter.load_models()
                    logger.info("‚úÖ ML compliance filter initialized")
                except Exception as e:
                    logger.error(f"‚ùå Failed to initialize ML filter: {e}")
            
            # Initialize OpenAI moderation
            if OPENAI_AVAILABLE:
                try:
                    self.openai_filter = OpenAIModerationFilter()
                    if self.openai_filter.enabled:
                        logger.info("‚úÖ OpenAI moderation initialized (95-98% accuracy mode)")
                    else:
                        logger.warning("‚ö†Ô∏è OpenAI API key not set. Set OPENAI_API_KEY for enhanced accuracy.")
                except Exception as e:
                    logger.error(f"‚ùå Failed to initialize OpenAI filter: {e}")
            
            # Initialize Privacy Detector
            if PRIVACY_DETECTOR_AVAILABLE:
                try:
                    self.privacy_detector = EnhancedPrivacyDetector()
                    logger.info("‚úÖ Enhanced privacy detector initialized (64+ PII patterns)")
                except Exception as e:
                    logger.error(f"‚ùå Failed to initialize privacy detector: {e}")
            
            self.is_initialized = True
            logger.info("üéâ Integrated system ready!")
            return True
        return False
    
    def analyze_content(self, text: str) -> Dict[str, Any]:
        """Comprehensive content analysis using all systems"""
        start_time = time.time()
        
        result = {
            'success': True,
            'timestamp': datetime.now().isoformat(),
            'content_length': len(text),
            'is_compliant': True,
            'overall_risk_score': 0.0,
            'threat_level': 'safe',
            'violations': [],
            'detections': {},
            'recommendations': [],
            'processing_time_ms': 0
        }
        
        # Initialize jailbreak tracking variables
        jailbreak_detected = False
        jailbreak_confidence = 0.0
        
        try:
            # 1. OpenAI Moderation (Primary - Highest Accuracy)
            if self.openai_filter and self.openai_filter.enabled:
                openai_result = self.openai_filter.moderate(text)
                result['detections']['openai'] = openai_result
                
                if openai_result['flagged']:
                    result['is_compliant'] = False
                    result['violations'].extend(openai_result['flagged_categories'])
                    result['overall_risk_score'] = max(
                        result['overall_risk_score'],
                        openai_result.get('highest_risk_score', 0.9)
                    )
                    self.metrics['openai_detections'] += 1
            
            # 2. Enhanced Jailbreak Detection (Secondary - Jailbreak Specific)
            # Returns three classifications: True (jailbreak), 'hate_speech' (hate speech only), False (safe)
            if self.enhanced_detector:
                jailbreak_result = self.enhanced_detector.analyze_enhanced(text)
                
                # Handle three-way classification
                is_jailbreak = jailbreak_result.is_jailbreak == True
                is_hate_speech_only = jailbreak_result.is_jailbreak == 'hate_speech'
                
                result['detections']['jailbreak'] = {
                    'detected': is_jailbreak,
                    'severity': jailbreak_result.severity.value,
                    'confidence': jailbreak_result.confidence,
                    'techniques': [t.value for t in jailbreak_result.techniques],
                    'explanation': jailbreak_result.explanation,
                    'patterns': jailbreak_result.patterns_detected[:5]
                }
                
                # Handle hate speech classification separately
                if is_hate_speech_only:
                    hate_speech_data = {
                        'detected': True,
                        'severity': jailbreak_result.severity.value,
                        'confidence': jailbreak_result.confidence,
                        'explanation': jailbreak_result.explanation,
                        'patterns': jailbreak_result.patterns_detected[:5]
                    }
                    result['detections']['hate_speech'] = hate_speech_data
                    
                    # Also add semantic_analysis for UI compatibility
                    if jailbreak_result.semantic_analysis:
                        result['detections']['semantic_analysis'] = jailbreak_result.semantic_analysis
                    
                    result['is_compliant'] = False
                    result['violations'].append('hate_speech')
                    result['overall_risk_score'] = max(result['overall_risk_score'], jailbreak_result.confidence)
                    self.metrics['jailbreak_attempts'] += 1  # Count hate speech in metrics
                    logger.info(f"üö® Hate speech detected: {jailbreak_result.confidence:.1%} confidence")
                
                # Token anomaly analysis
                if jailbreak_result.token_anomaly_analysis['has_anomalies']:
                    result['detections']['token_anomalies'] = {
                        'detected': True,
                        'anomalies': jailbreak_result.token_anomaly_analysis['anomalies'],
                        'score': jailbreak_result.token_anomaly_analysis['anomaly_score']
                    }
                    self.metrics['token_anomalies'] += 1
                
                # Update metrics - store for later smart filtering
                jailbreak_detected = is_jailbreak
                jailbreak_confidence = jailbreak_result.confidence
            
            # 3. Context-Specific Threat Detection
            from context_specific_threats import ContextSpecificThreatDetector
            context_detector = ContextSpecificThreatDetector()
            context_result = context_detector.detect_all_categories(text)
            
            context_threats_detected = []
            for threat in context_result:
                if threat.detected:
                    context_threats_detected.append({
                        'category': threat.category.value,
                        'severity': threat.severity,
                        'confidence': threat.confidence,
                        'indicators': len(threat.matched_patterns),
                        'explanation': threat.explanation
                    })
                    result['is_compliant'] = False
                    result['overall_risk_score'] = max(result['overall_risk_score'], threat.confidence)
            
            if context_threats_detected:
                result['context_threats'] = context_threats_detected
            
            # 4. Privacy Violation Detection
            privacy_has_violations = False
            if self.privacy_detector:
                try:
                    privacy_result = self.privacy_detector.detect(text)
                    result['detections']['privacy'] = {
                        'detected': privacy_result.has_violations,
                        'risk_level': privacy_result.risk_level,
                        'privacy_score': privacy_result.privacy_score,
                        'violations': [
                            {
                                'category': v.category.value,
                                'severity': v.severity,
                                'confidence': v.confidence,
                                'masked_value': v.masked_value
                            }
                            for v in privacy_result.violations[:5]  # Limit to first 5
                        ],
                        'explanation': privacy_result.explanation
                    }
                    
                    if privacy_result.has_violations:
                        privacy_has_violations = True
                        self.metrics['privacy_violations'] += 1
                        result['is_compliant'] = False
                        result['violations'].append('privacy_violation')
                        result['overall_risk_score'] = max(
                            result['overall_risk_score'],
                            privacy_result.privacy_score
                        )
                except Exception as e:
                    logger.error(f"Privacy detector error: {e}")
            
            # Smart filtering: Suppress jailbreak if more specific threat categories are detected
            # Priority: Context-Specific Threats > Privacy Violations > Jailbreak
            
            # If context-specific threats detected, suppress jailbreak (violence, school threats, etc. are more specific)
            if context_threats_detected and jailbreak_detected:
                # Context threats are ALWAYS more specific than generic jailbreak - suppress unless extremely high confidence
                jb_techniques = result['detections']['jailbreak'].get('techniques', [])
                violence_related = any('violence' in t.lower() for t in jb_techniques)
                
                if True:  # ALWAYS suppress jailbreak when context threat exists - it's more specific
                    # Context threat is more specific - suppress jailbreak
                    result['detections']['jailbreak']['detected'] = False
                    result['detections']['jailbreak']['confidence'] = 0.0
                    result['detections']['jailbreak']['explanation'] = f"Detection reclassified: Content flagged as context-specific threat ({', '.join([t['category'] for t in context_threats_detected])}), not a jailbreak attempt."
                    if 'jailbreak_attempt' in result['violations']:
                        result['violations'].remove('jailbreak_attempt')
                        self.metrics['jailbreak_attempts'] = max(0, self.metrics['jailbreak_attempts'] - 1)
                    jailbreak_detected = False
                    logger.info(f"üéØ Reclassified jailbreak as context threat: {', '.join([t['category'] for t in context_threats_detected])}")
            
            # If privacy violation detected, ALWAYS suppress jailbreak (PII triggers false positive obfuscation)
            if privacy_has_violations and jailbreak_detected:
                # Privacy violations (phone numbers, emails, etc.) trigger false positive "encoding" detections
                # ALWAYS suppress jailbreak when privacy is the real issue
                result['detections']['jailbreak']['detected'] = False
                result['detections']['jailbreak']['severity'] = 'low'
                result['detections']['jailbreak']['confidence'] = 0.0
                result['detections']['jailbreak']['explanation'] = 'Detection suppressed: Privacy violation detected. PII patterns incorrectly flagged as encoding obfuscation.'
                if 'jailbreak_attempt' in result['violations']:
                    result['violations'].remove('jailbreak_attempt')
                    self.metrics['jailbreak_attempts'] = max(0, self.metrics['jailbreak_attempts'] - 1)
                jailbreak_detected = False
                logger.info("üîí Suppressed jailbreak: Privacy violation detected (PII incorrectly flagged as obfuscation)")
            
            # Add jailbreak to violations if still detected after filtering
            if result['detections'].get('jailbreak', {}).get('detected'):
                if 'jailbreak_attempt' not in result['violations']:
                    self.metrics['jailbreak_attempts'] += 1
                    result['violations'].append('jailbreak_attempt')
                    result['is_compliant'] = False
                    result['overall_risk_score'] = max(
                        result['overall_risk_score'], 
                        jailbreak_confidence
                    )
            
            # 4. ML Compliance Filter (if available - DISABLED)
            if self.ml_filter:
                try:
                    ml_result = self.ml_filter.predict(text)
                    result['detections']['ml_compliance'] = {
                        'detected': ml_result.is_violation,
                        'confidence': ml_result.confidence,
                        'violation_type': ml_result.violation_type,
                        'severity': ml_result.severity,
                        'false_positive_likelihood': ml_result.false_positive_likelihood
                    }
                    
                    if ml_result.is_violation:
                        result['is_compliant'] = False
                        result['violations'].append(ml_result.violation_type)
                        result['overall_risk_score'] = max(
                            result['overall_risk_score'],
                            ml_result.confidence
                        )
                except Exception as e:
                    logger.error(f"ML filter error: {e}")
            
            # Determine threat level
            if result['overall_risk_score'] >= 0.9:
                result['threat_level'] = 'critical'
            elif result['overall_risk_score'] >= 0.75:
                result['threat_level'] = 'high'
            elif result['overall_risk_score'] >= 0.5:
                result['threat_level'] = 'moderate'
            elif result['overall_risk_score'] >= 0.3:
                result['threat_level'] = 'low'
            else:
                result['threat_level'] = 'safe'
            
            # Generate recommendations
            if not result['is_compliant']:
                if result['detections'].get('openai', {}).get('flagged'):
                    result['recommendations'].append(
                        "BLOCK: OpenAI Moderation flagged content. Industry-standard violation detected."
                    )
                if 'hate_speech' in result['violations']:
                    hate_det = result['detections'].get('hate_speech', {})
                    result['recommendations'].append(
                        f"BLOCK IMMEDIATELY: Hate speech detected with {hate_det.get('confidence', 0)*100:.1f}% confidence. Content promotes hatred or discrimination."
                    )
                if 'jailbreak_attempt' in result['violations']:
                    result['recommendations'].append(
                        "BLOCK: Critical jailbreak attempt detected. Do not process this request."
                    )
                if 'privacy_violation' in result['violations']:
                    privacy_det = result['detections'].get('privacy', {})
                    if privacy_det.get('risk_level') in ['critical', 'high']:
                        result['recommendations'].append(
                            f"BLOCK: Privacy violation detected. Content contains confidential information ({privacy_det.get('risk_level')} risk)."
                        )
                    else:
                        result['recommendations'].append(
                            "WARNING: Potential privacy violation detected. Review content before processing."
                        )
                if result['detections'].get('token_anomalies', {}).get('detected'):
                    result['recommendations'].append(
                        "WARNING: Token-level obfuscation detected. Potential evasion attempt."
                    )
            else:
                result['recommendations'].append("ALLOW: Content appears safe and compliant.")
            
            # Update metrics
            self.metrics['total_processed'] += 1
            
        except Exception as e:
            logger.error(f"Analysis error: {e}", exc_info=True)
            result['success'] = False
            result['error'] = str(e)
        
        # Calculate processing time
        result['processing_time_ms'] = int((time.time() - start_time) * 1000)
        
        # Update average response time (avoid division by zero)
        if self.metrics['total_processed'] > 0:
            self.metrics['avg_response_time'] = (
                (self.metrics['avg_response_time'] * (self.metrics['total_processed'] - 1) +
                 result['processing_time_ms']) / self.metrics['total_processed']
            )
        else:
            self.metrics['avg_response_time'] = result['processing_time_ms']
        
        return result
    
    def get_threat_intelligence_report(self) -> Dict[str, Any]:
        """Get comprehensive threat intelligence report"""
        if self.enhanced_detector:
            return self.enhanced_detector.get_threat_intelligence_report()
        return {}


# Initialize system
system = IntegratedSystem()


# HTML Template
HTML_TEMPLATE = '''
<!DOCTYPE html>
<html>
<head>
    <title>üöÄ Integrated Compliance Filter - Production</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
    <style>
        .gradient-bg { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); }
        .result-safe { border-left: 6px solid #28a745; background: #d4edda; }
        .result-violation { border-left: 6px solid #dc3545; background: #f8d7da; }
        .metric-card { border-left: 4px solid #667eea; transition: transform 0.2s; }
        .metric-card:hover { transform: translateY(-3px); }
        .feature-badge { font-size: 0.75rem; padding: 4px 8px; }
    </style>
</head>
<body class="bg-light">
    <nav class="navbar navbar-dark gradient-bg">
        <div class="container">
            <span class="navbar-brand">
                <i class="fas fa-shield-alt"></i> Integrated Compliance Filter
            </span>
        </div>
    </nav>

    <div class="container mt-4">
        <!-- Metrics Dashboard -->
        <div class="row mb-4">
            <div class="col-lg-4 col-md-6 mb-3">
                <div class="card metric-card text-center">
                    <div class="card-body">
                        <i class="fas fa-chart-line fa-2x text-primary mb-2"></i>
                        <h6>Total Analyzed</h6>
                        <h4 id="totalProcessed">{{ metrics.total_processed }}</h4>
                    </div>
                </div>
            </div>
            <div class="col-lg-4 col-md-6 mb-3">
                <div class="card metric-card text-center">
                    <div class="card-body">
                        <i class="fas fa-exclamation-triangle fa-2x text-danger mb-2"></i>
                        <h6>Jailbreaks Blocked</h6>
                        <h4 class="text-danger" id="jailbreakAttempts">{{ metrics.jailbreak_attempts }}</h4>
                    </div>
                </div>
            </div>
            <div class="col-lg-4 col-md-6 mb-3">
                <div class="card metric-card text-center">
                    <div class="card-body">
                        <i class="fas fa-tachometer-alt fa-2x text-success mb-2"></i>
                        <h6>Avg Response</h6>
                        <h4 class="text-success" id="avgResponse">{{ "%.0f"|format(metrics.avg_response_time) }}ms</h4>
                    </div>
                </div>
            </div>
        </div>

        <div class="row">
            <!-- Analysis Interface -->
            <div class="col-12">
                <div class="card shadow-sm">
                    <div class="card-header bg-white">
                        <h5 class="mb-0">
                            <i class="fas fa-microscope text-primary"></i> Advanced Content Analysis
                        </h5>
                    </div>
                    <div class="card-body">
                        <form id="analysisForm">
                            <div class="mb-3">
                                <label class="form-label fw-bold">Content to Analyze:</label>
                                <textarea class="form-control" id="contentInput" rows="6" 
                                         placeholder="Enter text for comprehensive analysis" 
                                         required></textarea>
                            </div>
                            <div class="d-grid">
                                <button type="submit" class="btn btn-primary btn-lg">
                                    <i class="fas fa-shield-alt"></i> Analyze Content
                                </button>
                            </div>
                        </form>
                        
                        <div id="result" class="mt-4" style="display: none;"></div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
    <script>
        document.getElementById('analysisForm').addEventListener('submit', async (e) => {
            e.preventDefault();
            const content = document.getElementById('contentInput').value;
            const resultDiv = document.getElementById('result');
            
            resultDiv.innerHTML = '<div class="text-center"><div class="spinner-border text-primary"></div><p class="mt-2">Analyzing...</p></div>';
            resultDiv.style.display = 'block';
            
            try {
                const response = await fetch('/api/analyze', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({text: content})
                });
                
                const result = await response.json();
                displayResult(result);
                updateMetrics();
            } catch (error) {
                resultDiv.innerHTML = '<div class="alert alert-danger">Error: ' + error.message + '</div>';
            }
        });

        function displayResult(result) {
            const resultDiv = document.getElementById('result');
            const isCompliant = result.is_compliant;
            const cardClass = isCompliant ? 'result-safe' : 'result-violation';
            
            let html = `
                <div class="card ${cardClass}">
                    <div class="card-body">
                        <h5>
                            <i class="fas fa-${isCompliant ? 'check-circle text-success' : 'exclamation-circle text-danger'}"></i>
                            ${isCompliant ? 'COMPLIANT' : 'VIOLATION DETECTED'}
                        </h5>
                        <p><strong>Threat Level:</strong> <span class="badge bg-${getThreatColor(result.threat_level)}">${result.threat_level.toUpperCase()}</span></p>
                        <p><strong>Risk Score:</strong> ${(result.overall_risk_score * 100).toFixed(1)}%</p>
                        <p><strong>Processing Time:</strong> ${result.processing_time_ms}ms</p>
            `;
            
            // OpenAI Detection
            if (result.detections.openai && result.detections.openai.flagged) {
                html += `
                    <div class="alert alert-danger mt-3">
                        <h6><i class="fas fa-robot"></i> OpenAI Moderation Alert</h6>
                        <p><strong>Categories:</strong> ${result.detections.openai.flagged_categories.join(', ')}</p>
                        <p><strong>Highest Risk:</strong> ${result.detections.openai.highest_risk_category} (${(result.detections.openai.highest_risk_score * 100).toFixed(1)}%)</p>
                        <p class="mb-0"><small>Detected by industry-standard ChatGPT moderation</small></p>
                    </div>
                `;
            }
            
            if (result.detections.jailbreak && result.detections.jailbreak.detected) {
                html += `
                    <div class="alert alert-warning mt-3">
                        <h6><i class="fas fa-skull-crossbones"></i> Jailbreak Detected</h6>
                        <p><strong>Severity:</strong> ${result.detections.jailbreak.severity}</p>
                        <p><strong>Confidence:</strong> ${(result.detections.jailbreak.confidence * 100).toFixed(1)}%</p>
                        <p><strong>Techniques:</strong> ${result.detections.jailbreak.techniques.join(', ')}</p>
                        <p class="mb-0"><small>${result.detections.jailbreak.explanation}</small></p>
                    </div>
                `;
            }
            
            if (result.detections.token_anomalies && result.detections.token_anomalies.detected) {
                html += `
                    <div class="alert alert-info mt-2">
                        <h6><i class="fas fa-search"></i> Token Anomalies Detected</h6>
                        <p class="mb-0">${result.detections.token_anomalies.anomalies.length} anomalies found</p>
                    </div>
                `;
            }
            
            html += `
                        <div class="mt-3">
                            <strong>Recommendations:</strong>
                            <ul class="mb-0">
                                ${result.recommendations.map(r => `<li>${r}</li>`).join('')}
                            </ul>
                        </div>
                    </div>
                </div>
            `;
            
            resultDiv.innerHTML = html;
        }

        function getThreatColor(level) {
            const colors = {
                'safe': 'success',
                'low': 'info',
                'moderate': 'warning',
                'high': 'danger',
                'critical': 'danger'
            };
            return colors[level] || 'secondary';
        }

        function testCase(text) {
            document.getElementById('contentInput').value = text;
            document.getElementById('analysisForm').dispatchEvent(new Event('submit'));
        }

        async function updateMetrics() {
            try {
                const response = await fetch('/api/metrics');
                const metrics = await response.json();
                
                document.getElementById('totalProcessed').textContent = metrics.total_processed;
                document.getElementById('jailbreakAttempts').textContent = metrics.jailbreak_attempts;
                document.getElementById('avgResponse').textContent = Math.round(metrics.avg_response_time) + 'ms';
            } catch (error) {
                console.error('Failed to update metrics:', error);
            }
        }

        // Update metrics every 5 seconds
        setInterval(updateMetrics, 5000);
    </script>
</body>
</html>
'''


# Routes
@app.route('/')
def index():
    """Main interface"""
    # Serve the professional UI
    try:
        with open('src/api/professional_ui.html', 'r', encoding='utf-8') as f:
            content = f.read()
            logger.info("‚úÖ Serving professional_ui.html (dynamic UI)")
            return content
    except FileNotFoundError as e:
        # Fallback to embedded template if file not found
        logger.warning(f"‚ö†Ô∏è Could not find professional_ui.html: {e}. Using fallback template.")
        openai_enabled = system.openai_filter and system.openai_filter.enabled if system.openai_filter else False
        return render_template_string(HTML_TEMPLATE, metrics=system.metrics, openai_enabled=openai_enabled)
    except Exception as e:
        logger.error(f"‚ùå Error loading professional_ui.html: {e}. Using fallback template.")
        openai_enabled = system.openai_filter and system.openai_filter.enabled if system.openai_filter else False
        return render_template_string(HTML_TEMPLATE, metrics=system.metrics, openai_enabled=openai_enabled)


@app.route('/api/analyze', methods=['POST'])
def analyze():
    """Analyze content"""
    try:
        data = request.get_json()
        text = data.get('text', '')
        
        if not text:
            return jsonify({'success': False, 'error': 'No text provided'}), 400
        
        result = system.analyze_content(text)
        return jsonify(result)
    
    except Exception as e:
        logger.error(f"Analysis error: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/metrics')
def metrics():
    """Get system metrics"""
    return jsonify(system.metrics)


@app.route('/api/threat-intelligence')
def threat_intelligence():
    """Get threat intelligence report"""
    try:
        report = system.get_threat_intelligence_report()
        return jsonify(report)
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/health')
def health():
    """Health check"""
    openai_enabled = system.openai_filter and system.openai_filter.enabled if system.openai_filter else False
    return jsonify({
        'status': 'healthy',
        'initialized': system.is_initialized,
        'enhanced_detector': system.enhanced_detector is not None,
        'ml_filter': system.ml_filter is not None,
        'openai_enabled': openai_enabled,
        'privacy_detector': system.privacy_detector is not None
    })


# ===== CONTEXT-SPECIFIC THREAT DETECTION APIs =====

@app.route('/api/detect/school-threat', methods=['POST'])
def detect_school_threat():
    """Detect school violence threats"""
    try:
        from context_specific_threats import ContextSpecificThreatDetector, ThreatCategory
        
        data = request.get_json()
        text = data.get('text', '')
        
        if not text:
            return jsonify({'error': 'No text provided'}), 400
        
        detector = ContextSpecificThreatDetector()
        result = detector.detect(text)
        
        # Filter to only school threats
        if result.detected and result.category == ThreatCategory.SCHOOL_THREAT:
            return jsonify({
                'detected': True,
                'category': 'school_threat',
                'severity': result.severity,
                'confidence': result.confidence,
                'explanation': result.explanation,
                'matched_patterns_count': len(result.matched_patterns)
            })
        
        return jsonify({'detected': False, 'category': 'school_threat'})
    
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/detect/self-harm', methods=['POST'])
def detect_self_harm():
    """Detect self-harm content including methods and encouragement"""
    try:
        from context_specific_threats import ContextSpecificThreatDetector, ThreatCategory
        
        data = request.get_json()
        text = data.get('text', '')
        
        if not text:
            return jsonify({'error': 'No text provided'}), 400
        
        detector = ContextSpecificThreatDetector()
        result = detector.detect(text)
        
        if result.detected and result.category == ThreatCategory.SELF_HARM:
            return jsonify({
                'detected': True,
                'category': 'self_harm',
                'severity': result.severity,
                'confidence': result.confidence,
                'explanation': result.explanation,
                'matched_patterns_count': len(result.matched_patterns),
                'warning': 'CRITICAL: Self-harm content detected. Immediate intervention may be required.'
            })
        
        return jsonify({'detected': False, 'category': 'self_harm'})
    
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/detect/drug-manufacturing', methods=['POST'])
def detect_drug_manufacturing():
    """Detect drug manufacturing instructions"""
    try:
        from context_specific_threats import ContextSpecificThreatDetector, ThreatCategory
        
        data = request.get_json()
        text = data.get('text', '')
        
        if not text:
            return jsonify({'error': 'No text provided'}), 400
        
        detector = ContextSpecificThreatDetector()
        result = detector.detect(text)
        
        if result.detected and result.category == ThreatCategory.DRUG_MANUFACTURING:
            return jsonify({
                'detected': True,
                'category': 'drug_manufacturing',
                'severity': result.severity,
                'confidence': result.confidence,
                'explanation': result.explanation,
                'matched_patterns_count': len(result.matched_patterns)
            })
        
        return jsonify({'detected': False, 'category': 'drug_manufacturing'})
    
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/detect/weapons', methods=['POST'])
def detect_weapons():
    """Detect specific weapons and explosives"""
    try:
        from context_specific_threats import ContextSpecificThreatDetector, ThreatCategory
        
        data = request.get_json()
        text = data.get('text', '')
        
        if not text:
            return jsonify({'error': 'No text provided'}), 400
        
        detector = ContextSpecificThreatDetector()
        result = detector.detect(text)
        
        if result.detected and result.category == ThreatCategory.SPECIFIC_WEAPON:
            return jsonify({
                'detected': True,
                'category': 'specific_weapon',
                'severity': result.severity,
                'confidence': result.confidence,
                'explanation': result.explanation,
                'matched_patterns_count': len(result.matched_patterns)
            })
        
        return jsonify({'detected': False, 'category': 'specific_weapon'})
    
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/detect/workplace-violence', methods=['POST'])
def detect_workplace_violence():
    """Detect workplace violence threats"""
    try:
        from context_specific_threats import ContextSpecificThreatDetector, ThreatCategory
        
        data = request.get_json()
        text = data.get('text', '')
        
        if not text:
            return jsonify({'error': 'No text provided'}), 400
        
        detector = ContextSpecificThreatDetector()
        result = detector.detect(text)
        
        if result.detected and result.category == ThreatCategory.WORKPLACE_VIOLENCE:
            return jsonify({
                'detected': True,
                'category': 'workplace_violence',
                'severity': result.severity,
                'confidence': result.confidence,
                'explanation': result.explanation,
                'matched_patterns_count': len(result.matched_patterns)
            })
        
        return jsonify({'detected': False, 'category': 'workplace_violence'})
    
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/detect/target-threat', methods=['POST'])
def detect_target_threat():
    """Detect target-specific threats (government, mass casualty)"""
    try:
        from context_specific_threats import ContextSpecificThreatDetector, ThreatCategory
        
        data = request.get_json()
        text = data.get('text', '')
        
        if not text:
            return jsonify({'error': 'No text provided'}), 400
        
        detector = ContextSpecificThreatDetector()
        result = detector.detect(text)
        
        if result.detected and result.category == ThreatCategory.TARGET_THREAT:
            return jsonify({
                'detected': True,
                'category': 'target_threat',
                'severity': result.severity,
                'confidence': result.confidence,
                'explanation': result.explanation,
                'matched_patterns_count': len(result.matched_patterns)
            })
        
        return jsonify({'detected': False, 'category': 'target_threat'})
    
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/detect/sexual-violence', methods=['POST'])
def detect_sexual_violence():
    """Detect sexual violence and exploitation content"""
    try:
        from context_specific_threats import ContextSpecificThreatDetector, ThreatCategory
        
        data = request.get_json()
        text = data.get('text', '')
        
        if not text:
            return jsonify({'error': 'No text provided'}), 400
        
        detector = ContextSpecificThreatDetector()
        result = detector.detect(text)
        
        if result.detected and result.category == ThreatCategory.SEXUAL_VIOLENCE:
            return jsonify({
                'detected': True,
                'category': 'sexual_violence',
                'severity': result.severity,
                'confidence': result.confidence,
                'explanation': result.explanation,
                'matched_patterns_count': len(result.matched_patterns),
                'warning': 'CRITICAL: Sexual violence content detected.'
            })
        
        return jsonify({'detected': False, 'category': 'sexual_violence'})
    
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/detect/medical-misinformation', methods=['POST'])
def detect_medical_misinformation():
    """Detect medical misinformation and dangerous health advice"""
    try:
        from context_specific_threats import ContextSpecificThreatDetector, ThreatCategory
        
        data = request.get_json()
        text = data.get('text', '')
        
        if not text:
            return jsonify({'error': 'No text provided'}), 400
        
        detector = ContextSpecificThreatDetector()
        result = detector.detect(text)
        
        if result.detected and result.category == ThreatCategory.MEDICAL_MISINFORMATION:
            return jsonify({
                'detected': True,
                'category': 'medical_misinformation',
                'severity': result.severity,
                'confidence': result.confidence,
                'explanation': result.explanation,
                'matched_patterns_count': len(result.matched_patterns)
            })
        
        return jsonify({'detected': False, 'category': 'medical_misinformation'})
    
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/detect/explicit-content', methods=['POST'])
def detect_explicit_content():
    """Detect explicit sexual content generation requests"""
    try:
        from context_specific_threats import ContextSpecificThreatDetector, ThreatCategory
        
        data = request.get_json()
        text = data.get('text', '')
        
        if not text:
            return jsonify({'error': 'No text provided'}), 400
        
        detector = ContextSpecificThreatDetector()
        result = detector.detect(text)
        
        if result.detected and result.category == ThreatCategory.EXPLICIT_SEXUAL_CONTENT:
            return jsonify({
                'detected': True,
                'category': 'explicit_sexual_content',
                'severity': result.severity,
                'confidence': result.confidence,
                'explanation': result.explanation,
                'matched_patterns_count': len(result.matched_patterns)
            })
        
        return jsonify({'detected': False, 'category': 'explicit_sexual_content'})
    
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/detect/privacy', methods=['POST'])
def detect_privacy():
    """Detect privacy violations and PII in content"""
    try:
        data = request.get_json()
        text = data.get('text', '')
        
        if not text:
            return jsonify({'error': 'No text provided'}), 400
        
        if not system.privacy_detector:
            return jsonify({'error': 'Privacy detector not available'}), 503
        
        result = system.privacy_detector.detect(text)
        
        return jsonify({
            'detected': result.has_violations,
            'risk_level': result.risk_level,
            'privacy_score': result.privacy_score,
            'violations': [
                {
                    'category': v.category.value,
                    'severity': v.severity,
                    'confidence': v.confidence,
                    'masked_value': v.masked_value,
                    'description': v.description
                }
                for v in result.violations
            ],
            'explanation': result.explanation,
            'total_violations': len(result.violations)
        })
    
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/categories')
def list_categories():
    """List all available threat detection categories"""
    return jsonify({
        'categories': [
            {
                'name': 'school_threat',
                'endpoint': '/api/detect/school-threat',
                'severity': 'critical',
                'description': 'School and campus violence threats'
            },
            {
                'name': 'self_harm',
                'endpoint': '/api/detect/self-harm',
                'severity': 'critical',
                'description': 'Self-harm methods and encouragement'
            },
            {
                'name': 'drug_manufacturing',
                'endpoint': '/api/detect/drug-manufacturing',
                'severity': 'high',
                'description': 'Drug production instructions'
            },
            {
                'name': 'specific_weapon',
                'endpoint': '/api/detect/weapons',
                'severity': 'high',
                'description': 'Specific weapons and explosives'
            },
            {
                'name': 'workplace_violence',
                'endpoint': '/api/detect/workplace-violence',
                'severity': 'critical',
                'description': 'Workplace violence threats'
            },
            {
                'name': 'target_threat',
                'endpoint': '/api/detect/target-threat',
                'severity': 'critical',
                'description': 'Government and mass casualty targets'
            },
            {
                'name': 'sexual_violence',
                'endpoint': '/api/detect/sexual-violence',
                'severity': 'critical',
                'description': 'Sexual violence and exploitation'
            },
            {
                'name': 'medical_misinformation',
                'endpoint': '/api/detect/medical-misinformation',
                'severity': 'high',
                'description': 'Dangerous medical advice and misinformation'
            },
            {
                'name': 'explicit_sexual_content',
                'endpoint': '/api/detect/explicit-content',
                'severity': 'high',
                'description': 'Explicit sexual content generation'
            },
            {
                'name': 'privacy_violation',
                'endpoint': '/api/detect/privacy',
                'severity': 'critical',
                'description': 'PII and confidential information (20 categories)'
            }
        ],
        'total_categories': 10,
        'general_endpoint': '/api/analyze'
    })


if __name__ == '__main__':
    print("üöÄ Starting Integrated Compliance Filter Server...")
    print("=" * 80)
    
    # Initialize system
    system.initialize()
    
    print("\n‚úÖ Server ready!")
    print("üåê Web Interface: http://localhost:5000")
    print("\nüì° API Endpoints:")
    print("   General Analysis: http://localhost:5000/api/analyze")
    print("   List Categories: http://localhost:5000/api/categories")
    print("\nüéØ Threat Detection APIs:")
    print("   School Threats: http://localhost:5000/api/detect/school-threat")
    print("   Self-Harm: http://localhost:5000/api/detect/self-harm")
    print("   Drug Manufacturing: http://localhost:5000/api/detect/drug-manufacturing")
    print("   Weapons: http://localhost:5000/api/detect/weapons")
    print("   Workplace Violence: http://localhost:5000/api/detect/workplace-violence")
    print("   Target Threats: http://localhost:5000/api/detect/target-threat")
    print("   Sexual Violence: http://localhost:5000/api/detect/sexual-violence")
    print("   Medical Misinfo: http://localhost:5000/api/detect/medical-misinformation")
    print("   Explicit Content: http://localhost:5000/api/detect/explicit-content")
    print("   Privacy/PII: http://localhost:5000/api/detect/privacy")
    print("\nüìä System:")
    print("   Metrics: http://localhost:5000/api/metrics")
    print("   Threat Intel: http://localhost:5000/api/threat-intelligence")
    print("   Health: http://localhost:5000/health")
    print("\n" + "=" * 80)
    
    # Start server
    app.run(debug=False, host='0.0.0.0', port=5000)
